{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Matthew Karani\n",
    "* Student pace: Full Time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Nikita Njoroge\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - remember to use markdown cells for comments as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "STEP 1 : Problem Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "STEP 2 : Data Collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Extracting the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import zipfile"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T12:47:28.431944Z",
     "start_time": "2024-03-20T12:47:28.426390Z"
    }
   },
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:12:12.293041Z",
     "start_time": "2024-03-20T13:12:12.287010Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: Group the specific file paths according to the file extension\n",
    "         eg by .csv, .db , .tsv etc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#defining the file paths\n",
    "zipped_file_paths = [\n",
    "    'zippedData/bom.movie_gross.csv.gz',\n",
    "    'zippedData/im.db.zip',\n",
    "    'zippedData/rt.movie_info.tsv.gz',\n",
    "    'zippedData/rt.reviews.tsv.gz',\n",
    "    'zippedData/tmdb.movies.csv.gz',\n",
    "    'zippedData/tn.movie_budgets.csv.gz'   \n",
    "]\n",
    "\n",
    "csv_files = []\n",
    "tsv_files = []\n",
    "sql_files = []\n",
    "\n",
    "for file_path in zipped_file_paths:\n",
    "    if '.csv' in file_path:\n",
    "        csv_files.append(file_path)\n",
    "             \n",
    "    elif '.tsv' in file_path:\n",
    "        tsv_files.append(file_path)\n",
    "        \n",
    "    elif '.zip' in file_path and '.db' in file_path:\n",
    "        sql_files.append(file_path)\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:21:32.988252Z",
     "start_time": "2024-03-20T13:21:32.978938Z"
    }
   },
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are CSV files ['zippedData/bom.movie_gross.csv.gz', 'zippedData/tmdb.movies.csv.gz', 'zippedData/tn.movie_budgets.csv.gz']\n",
      "These are TSV files ['zippedData/rt.movie_info.tsv.gz', 'zippedData/rt.reviews.tsv.gz'] \n",
      "These are SQl database files ['zippedData/im.db.zip']\n"
     ]
    }
   ],
   "source": [
    "print(f\"These are CSV files {csv_files}\")\n",
    "print(f\"These are TSV files {tsv_files} \")\n",
    "print(f\"These are SQl database files {sql_files}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:33:48.115764Z",
     "start_time": "2024-03-20T13:33:48.108344Z"
    }
   },
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the files per the specific type of file path using for loops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for file: zippedData/rt.movie_info.tsv.gz:\n",
      "   id                                           synopsis rating  \\\n",
      "0   1  This gritty, fast-paced, and innovative police...      R   \n",
      "1   3  New York City, not-too-distant-future: Eric Pa...      R   \n",
      "2   5  Illeana Douglas delivers a superb performance ...      R   \n",
      "3   6  Michael Douglas runs afoul of a treacherous su...      R   \n",
      "4   7                                                NaN     NR   \n",
      "\n",
      "                                 genre          director  \\\n",
      "0  Action and Adventure|Classics|Drama  William Friedkin   \n",
      "1    Drama|Science Fiction and Fantasy  David Cronenberg   \n",
      "2    Drama|Musical and Performing Arts    Allison Anders   \n",
      "3           Drama|Mystery and Suspense    Barry Levinson   \n",
      "4                        Drama|Romance    Rodney Bennett   \n",
      "\n",
      "                            writer  theater_date      dvd_date currency  \\\n",
      "0                   Ernest Tidyman   Oct 9, 1971  Sep 25, 2001      NaN   \n",
      "1     David Cronenberg|Don DeLillo  Aug 17, 2012   Jan 1, 2013        $   \n",
      "2                   Allison Anders  Sep 13, 1996  Apr 18, 2000      NaN   \n",
      "3  Paul Attanasio|Michael Crichton   Dec 9, 1994  Aug 27, 1997      NaN   \n",
      "4                     Giles Cooper           NaN           NaN      NaN   \n",
      "\n",
      "  box_office      runtime             studio  \n",
      "0        NaN  104 minutes                NaN  \n",
      "1    600,000  108 minutes  Entertainment One  \n",
      "2        NaN  116 minutes                NaN  \n",
      "3        NaN  128 minutes                NaN  \n",
      "4        NaN  200 minutes                NaN  \n",
      "\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 6558: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[89], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# #load tsv files into dataframes\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_path \u001B[38;5;129;01min\u001B[39;00m tsv_files:\n\u001B[0;32m      3\u001B[0m       \u001B[38;5;66;03m# Load data into dataframe\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\t\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgzip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Clean data (example: removing duplicates)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     df\u001B[38;5;241m.\u001B[39mdrop_duplicates(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Movie_Reviews_Analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Movie_Reviews_Analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Movie_Reviews_Analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Movie_Reviews_Analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2053\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xa0 in position 6558: invalid start byte"
     ]
    }
   ],
   "source": [
    "# #load tsv files into dataframes\n",
    "for file_path in tsv_files:\n",
    "      # Load data into dataframe\n",
    "    df = pd.read_csv(file_path, sep='\\t', compression='gzip')\n",
    "    \n",
    "    # Clean data (example: removing duplicates)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Display the cleaned dataframe\n",
    "    print(f\"DataFrame for file: {file_path}:\")\n",
    "    print(df.head())  # Display first few rows of the cleaned dataframe\n",
    "    print(\"\\n\")  # Print a separator between dataframes\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T14:14:33.744885Z",
     "start_time": "2024-03-20T14:14:33.488387Z"
    }
   },
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for file: zippedData/bom.movie_gross.csv.gz:\n",
      "                                         title studio  domestic_gross  \\\n",
      "0                                  Toy Story 3     BV     415000000.0   \n",
      "1                   Alice in Wonderland (2010)     BV     334200000.0   \n",
      "2  Harry Potter and the Deathly Hallows Part 1     WB     296000000.0   \n",
      "3                                    Inception     WB     292600000.0   \n",
      "4                          Shrek Forever After   P/DW     238700000.0   \n",
      "\n",
      "  foreign_gross  year  \n",
      "0     652000000  2010  \n",
      "1     691300000  2010  \n",
      "2     664300000  2010  \n",
      "3     535700000  2010  \n",
      "4     513900000  2010  \n",
      "\n",
      "\n",
      "DataFrame for file: zippedData/tmdb.movies.csv.gz:\n",
      "   Unnamed: 0            genre_ids     id original_language  \\\n",
      "0           0      [12, 14, 10751]  12444                en   \n",
      "1           1  [14, 12, 16, 10751]  10191                en   \n",
      "2           2        [12, 28, 878]  10138                en   \n",
      "3           3      [16, 35, 10751]    862                en   \n",
      "4           4        [28, 878, 12]  27205                en   \n",
      "\n",
      "                                 original_title  popularity release_date  \\\n",
      "0  Harry Potter and the Deathly Hallows: Part 1      33.533   2010-11-19   \n",
      "1                      How to Train Your Dragon      28.734   2010-03-26   \n",
      "2                                    Iron Man 2      28.515   2010-05-07   \n",
      "3                                     Toy Story      28.005   1995-11-22   \n",
      "4                                     Inception      27.920   2010-07-16   \n",
      "\n",
      "                                          title  vote_average  vote_count  \n",
      "0  Harry Potter and the Deathly Hallows: Part 1           7.7       10788  \n",
      "1                      How to Train Your Dragon           7.7        7610  \n",
      "2                                    Iron Man 2           6.8       12368  \n",
      "3                                     Toy Story           7.9       10174  \n",
      "4                                     Inception           8.3       22186  \n",
      "\n",
      "\n",
      "DataFrame for file: zippedData/tn.movie_budgets.csv.gz:\n",
      "   id  release_date                                        movie  \\\n",
      "0   1  Dec 18, 2009                                       Avatar   \n",
      "1   2  May 20, 2011  Pirates of the Caribbean: On Stranger Tides   \n",
      "2   3   Jun 7, 2019                                 Dark Phoenix   \n",
      "3   4   May 1, 2015                      Avengers: Age of Ultron   \n",
      "4   5  Dec 15, 2017            Star Wars Ep. VIII: The Last Jedi   \n",
      "\n",
      "  production_budget domestic_gross worldwide_gross  \n",
      "0      $425,000,000   $760,507,625  $2,776,345,279  \n",
      "1      $410,600,000   $241,063,875  $1,045,663,875  \n",
      "2      $350,000,000    $42,762,350    $149,762,350  \n",
      "3      $330,600,000   $459,005,868  $1,403,013,963  \n",
      "4      $317,000,000   $620,181,382  $1,316,721,747  \n"
     ]
    }
   ],
   "source": [
    "#Load csv files into dataframes\n",
    "for file_path in csv_files:\n",
    "    \n",
    "    # Load data into dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Clean data (example: removing duplicates)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Display the cleaned dataframe\n",
    "    print(f\"DataFrame for file: {file_path}:\")\n",
    "    print(df.head())  # Display first few rows of the cleaned dataframe\n",
    "    print(\"\\n\")  # Print a separator between dataframes\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #load sql files into a dataframes\n",
    "# for file_path in sql_files:\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:39:34.824736Z",
     "start_time": "2024-03-20T13:39:34.657627Z"
    }
   },
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the first five rows in the files "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Csv files\n",
    "\n",
    "\n",
    "# Tsv Files\n",
    "\n",
    "\n",
    "#Sql files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean the data\n",
    "1) Check for missing values.\n",
    "2) Check for null values\n",
    "3) Check for duplicates "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Clean Missing Values \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check for Duplicates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Check for duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3 : Data Preparation (Analysis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4: Exploratory Data Analysis(EDA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: Feature Selection and Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "//Future Steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6 : Model Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 7 : Model Evaluation "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 8 : Model Tuning(Hyperparameter Optimization)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 9 : Deployment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 10 : Monitoring and Maintanance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 11 : Documentation and Reporting"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
